{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependency \n",
    "import numpy as np \n",
    "import torch \n",
    "import torch.optim as optim\n",
    "import torch.nn as nn \n",
    "# Import the tensorboard used dependency\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generation \n",
    "true_b = 1\n",
    "true_w = 2 \n",
    "N = 100\n",
    "\n",
    "# set the random seed for numpy \n",
    "np.random.seed(43)\n",
    "\n",
    "x= np.random.rand(N,1)\n",
    "epsilon = (.1 * np.random.rand(N,1))\n",
    "\n",
    "y = true_b + true_w *x + epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate training and validating sets\n",
    "idx = np.arange(N)\n",
    "\n",
    "# Use first 80 random indices for train \n",
    "train_idx = idx[:int(N*.8)]\n",
    "val_idx = idx[int(N*.8):]\n",
    "\n",
    "# Generate train and validation sets\n",
    "x_train, y_train = x[train_idx], y[train_idx]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation \n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Transform data from numpy array to torch tensor\n",
    "x_train_tensor = torch.as_tensor(x_train).float().to(device)\n",
    "y_train_tensor = torch.as_tensor(y_train).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([0.1151]), tensor([1.2404]))\n",
      "80\n"
     ]
    }
   ],
   "source": [
    "# Build a Dataset\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, x_tensor, y_tensor):\n",
    "        self.x = x_tensor\n",
    "        self.y = y_tensor\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return (self.x[index], self.y[index])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "train_data = CustomDataset(x_train_tensor, y_train_tensor)\n",
    "print(train_data[0])\n",
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n"
     ]
    }
   ],
   "source": [
    "#Use the `random split` to split data\n",
    "import torch.utils.data as data \n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "#Build tensors from numpy arrays BEfore split\n",
    "x_tensor = torch.as_tensor(x).float()\n",
    "y_tensor = torch.as_tensor(y).float()\n",
    "\n",
    "# Build the datasets containing all data points\n",
    "dataset = CustomDataset(x_tensor, y_tensor)\n",
    "\n",
    "# Performs the split\n",
    "ratio = .8\n",
    "n_total = len(dataset)\n",
    "n_train = int(n_total*ratio)\n",
    "n_val = n_total - n_train\n",
    "\n",
    "train_data, val_data = data.random_split(dataset, [n_train, n_val])\n",
    "\n",
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7fe0c814e520>\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Build a data loader that yields mini-batches of size 2\n",
    "train_loader = DataLoader(\n",
    "        dataset=train_data,\n",
    "        batch_size=2,\n",
    "        shuffle=True\n",
    ")\n",
    "print(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data loader for the validation set\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_data,\n",
    "    batch_size=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[0.6953],\n",
      "        [0.0569]]), tensor([[2.4350],\n",
      "        [1.1967]])]\n"
     ]
    }
   ],
   "source": [
    "print(next(iter(train_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the train step\n",
    "\n",
    "def make_train_step(model, loss_fn, optimizer):\n",
    "    # Builds function that performs a step in the train loop \n",
    "    def perform_train_step(x,y):\n",
    "        \n",
    "        # Set the model to TRAIN mode\n",
    "        model.train()\n",
    "        \n",
    "        # Step1: Compute the model's predicition - forward pass\n",
    "        yhat = model(x)\n",
    "        \n",
    "        # Step2: Compute the loss\n",
    "        loss = loss_fn(yhat, y)\n",
    "        \n",
    "        # Step3: Compute gradients for \"b\" and \"w\" parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # Step4: Updates parameters using gradients and the learning rate\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #print(model.state_dict())\n",
    "        # Return the loss \n",
    "        return loss.item()\n",
    "    #Return the function that will be called inside the train loop\n",
    "    return perform_train_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the validation step \n",
    "def make_val_step(model, loss_fn):\n",
    "    # Build function that perform a step in the validation loop\n",
    "    def perform_val_step(x,y):\n",
    "        # Set the model to evaluation mode\n",
    "        model.eval()\n",
    "        \n",
    "        # Step 1: Compute the model's prediciton-forward pass\n",
    "        yhat = model(x)\n",
    "        \n",
    "        # Step 2: Compute the loss\n",
    "        loss = loss_fn(yhat, y)\n",
    "        \n",
    "        # We don't need to compute gradients and update the parameter here\n",
    "        # since we don't update parameters during evaluation.\n",
    "        return loss.item()\n",
    "    return perform_val_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('linear.weight', tensor([[0.7645]])), ('linear.bias', tensor([0.8300]))])\n"
     ]
    }
   ],
   "source": [
    "# Model config \n",
    "\n",
    "# Define the model \n",
    "class ManualLinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1,1)\n",
    "    def forward(self,x):\n",
    "        return self.linear(x)\n",
    "\n",
    "\n",
    "# Set learning rate \n",
    "lr = 0.1\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Create a model and send it to the device \n",
    "model = ManualLinearRegression().to(device)\n",
    "print(model.state_dict())\n",
    "\n",
    "# Define a SGD optimizer to update the parameters \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "# Define a MSE loss function \n",
    "loss_fn = nn.MSELoss(reduction=\"mean\")\n",
    "\n",
    "\n",
    "# Create a train_step \n",
    "train_step = make_train_step(model, loss_fn, optimizer)\n",
    "\n",
    "# Create a val_step\n",
    "val_step = make_val_step(model, loss_fn)\n",
    "\n",
    "# Create a summary writer to interface with Tensorboard\n",
    "writer = SummaryWriter('runs/simple_linear_regression')\n",
    "\n",
    "# Add graph\n",
    "# Fetch tuple of feature and label \n",
    "dummy_x, dummy_y = next(iter(train_loader))\n",
    "\n",
    "# Since our model was sent to device, we need to do the same\n",
    "writer.add_graph(model, dummy_x.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a function for mini_batch\n",
    "def mini_batch(device, dataloader, step):\n",
    "    \"\"\"A function th do mini-batch training.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "        device: where to send the data\n",
    "        dataloader: draw the mini-batch\n",
    "        step: the training step fucntion\n",
    "    \"\"\"\n",
    "    \n",
    "    mini_batch_losses = []\n",
    "    for x_batch, y_batch in dataloader:\n",
    "        # Send the mini-batch data to the device\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_bacth = y_batch.to(device)\n",
    "        \n",
    "        # Perform the train step\n",
    "        mini_batch_loss = step(x_batch, y_batch)\n",
    "        mini_batch_losses.append(mini_batch_loss) \n",
    "    \n",
    "    #Compute the average loss over all mini-batches\n",
    "    loss = np.mean(mini_batch_losses)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training\n",
    "\n",
    "n_epochs = 1000\n",
    "losses = []\n",
    "val_losses = []\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    # Call the mini batch function\n",
    "    loss = mini_batch(device, train_loader, train_step)\n",
    "    losses.append(loss)\n",
    "    \n",
    "    # Validation - no gradients in validation\n",
    "    # Use `torch.no_grad()` this context manager \n",
    "    # to disable any gradient computation\n",
    "    with torch.no_grad():\n",
    "        val_loss = mini_batch(device, val_loader, val_step)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "    # Add scalars \n",
    "    writer.add_scalars(\n",
    "       main_tag = 'loss',\n",
    "       tag_scalar_dict = {\n",
    "           'training': loss,\n",
    "           'validation': val_loss\n",
    "       },\n",
    "        global_step=epoch)\n",
    "\n",
    "#close the writer\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('linear.weight', tensor([[1.9951]])), ('linear.bias', tensor([1.0443]))])\n"
     ]
    }
   ],
   "source": [
    "# Model's parameter before training \n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tensorboard notebook extension\n",
    "%load_ext tensorboard "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-b57dc0a770902ad9\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-b57dc0a770902ad9\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6007;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Running the Tensorboard notebook extension\n",
    "%tensorboard --logdir runs/simple_linear_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check model's parameters\n",
    "#print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[2.0054]], requires_grad=True), Parameter containing:\n",
      "tensor([1.0554], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "print(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.013594414107501507, 0.00614015688188374, 0.0029982510022819042, 0.0017715154099278152, 0.0013672834902536124, 0.0014056369254831225, 0.001380442758090794, 0.0013766773045063019, 0.0009196145401801914, 0.001146456750575453, 0.0010036606690846384, 0.0012659892381634563, 0.0009606099338270724, 0.0011863455292768776, 0.0010928059346042573, 0.0009636757604312152, 0.0009554814605508, 0.001381834561470896, 0.0011494726059027016, 0.0008877780055627227, 0.0016226691077463329, 0.0015697689959779382, 0.0008746515086386353, 0.0013831548858433962, 0.0011265744105912745, 0.001196929399156943, 0.001670920115429908, 0.0009932741522789001, 0.0015094078262336552, 0.001701157249044627, 0.0010661108826752752, 0.0010211825428996235, 0.0010415800788905472, 0.001434624195098877, 0.0014004451804794371, 0.0013494285522028804, 0.0011520017578732222, 0.0017580457497388124, 0.0011886984575539827, 0.0015203332877717912, 0.0010357198189012706, 0.0010937144106719643, 0.0013684626901522279, 0.001031870546285063, 0.0018790041795000434, 0.0011967041937168688, 0.0016995479818433523, 0.0009354366338811815, 0.0010944058594759554, 0.001058779307641089, 0.0009302694525104016, 0.0015814909129403532, 0.000886292225914076, 0.001118512562243268, 0.001044958276906982, 0.0011165645264554769, 0.000999574753222987, 0.0011329014087095857, 0.0013124897959642112, 0.0009049345680978149, 0.0017350413836538792, 0.001271391607588157, 0.0009153579303529114, 0.0010458153847139329, 0.001539784949272871, 0.0012679195788223296, 0.0013659162214025855, 0.001163031323812902, 0.00117133604362607, 0.001205305045004934, 0.0009288179280702025, 0.001351720653474331, 0.0009771173645276576, 0.0009295155759900808, 0.0009249166178051382, 0.001612542022485286, 0.0009242003434337676, 0.0011405503028072417, 0.0014052546466700733, 0.0010989004513248801, 0.0012854517553932965, 0.0013656510855071247, 0.0008856748172547668, 0.0017627158085815609, 0.001151671924162656, 0.0009509731607977301, 0.0008769086271058768, 0.0009476079139858484, 0.0009991599945351481, 0.0010531421285122633, 0.0010659197578206658, 0.0010389020026195794, 0.0015872516087256372, 0.002258579188492149, 0.0009723049006424844, 0.002496264292858541, 0.0013139782822690904, 0.0012581107439473271, 0.0009095532586798072, 0.0009276957134716213, 0.001438870094716549, 0.001181058760266751, 0.0011443798139225692, 0.0010354459809605032, 0.0015446494799107313, 0.001349626516457647, 0.0009993460262194276, 0.001289050152990967, 0.0011919807002414018, 0.0014690380776301026, 0.0010918992920778692, 0.0008852680039126426, 0.0009493075194768608, 0.0011368678242433816, 0.0011478290834929794, 0.0010959773790091276, 0.0009717440989334136, 0.001231408939929679, 0.0014253699337132275, 0.0010664898727554828, 0.0014501669793389738, 0.0010637972154654562, 0.0010847169905900955, 0.0019039162434637547, 0.001082356320694089, 0.0010847151570487767, 0.0009565681684762239, 0.0015105982893146574, 0.0013223762507550418, 0.0011792743462137878, 0.001059689704561606, 0.0010771660890895873, 0.001553166308440268, 0.0014077132800593972, 0.0017217725981026888, 0.0010508708364795893, 0.0012377869570627809, 0.0013995469198562205, 0.0010856965382117778, 0.0018432986689731479, 0.0009324645798187703, 0.0009734149789437652, 0.0009661211224738508, 0.0012178404431324452, 0.001319501781836152, 0.0013024291838519275, 0.001160106941824779, 0.0010161317186430097, 0.0009849962079897523, 0.001172497111838311, 0.001215859956573695, 0.0011505154252517968, 0.0010791801032610238, 0.0020362638169899583, 0.0008790010761003941, 0.0010340205335523933, 0.0008801081276033074, 0.001435666054021567, 0.0011034363706130534, 0.0009474890248384327, 0.0012525488855317235, 0.0012797004892490804, 0.0009100968309212476, 0.0010992904426530004, 0.0010579756344668567, 0.000904170039575547, 0.0009717163047753274, 0.0012257543567102402, 0.0014446530840359628, 0.000882576045114547, 0.0015141221228986979, 0.00090669552446343, 0.0014717422309331596, 0.0009375367371831089, 0.0011978705879300833, 0.001828667416702956, 0.0009661342191975564, 0.0010738468554336578, 0.001339055655989796, 0.0010830312385223806, 0.0008792193839326501, 0.0012780018150806427, 0.0010912383731920272, 0.001148126379121095, 0.0009062049794010818, 0.0013515172176994383, 0.0009221188083756715, 0.0012448183842934668, 0.0013472697464749217, 0.0013964640093035996, 0.0011413766478653997, 0.0009780424588825554, 0.001285273174289614, 0.0011100524279754609, 0.0016886250814422965, 0.0010761345620267093, 0.0010896781168412417, 0.0009473054378759116, 0.0013567933929152787, 0.0012418577680364251, 0.0008881028625182807, 0.0016153103206306696, 0.0010100499785039574, 0.0013613025657832623, 0.0015472878003492951, 0.001432308868970722, 0.0016577619244344532, 0.001172014104668051, 0.0009256774210371077, 0.0015917917480692267, 0.0010100859217345715, 0.0012559571769088507, 0.0015119683812372386, 0.0014071554178372025, 0.0014732927666045725, 0.0014447604771703482, 0.0010946565307676792, 0.0011158995039295405, 0.0013045633095316589, 0.000923941726796329, 0.002024996210820973, 0.0009183327492792159, 0.00122856127563864, 0.0016531574074178934, 0.001221140322741121, 0.001021630858303979, 0.0011235414422117174, 0.0018078749999403954, 0.0009237030753865838, 0.0012568893725983799, 0.0011724378273356706, 0.0012898853165097535, 0.0010304399474989623, 0.001620849419850856, 0.0014042252441868186, 0.001293385896133259, 0.0014683828339911997, 0.0013301832950673997, 0.0011835050245281309, 0.00123901991173625, 0.0010217430535703897, 0.001217198936501518, 0.001196572120534256, 0.0009738010994624346, 0.0012774850765708834, 0.0013282186118885875, 0.000884929730091244, 0.001109122415073216, 0.0017211948288604617, 0.0012269689759705216, 0.0014475162024609745, 0.0013019150937907398, 0.0011657345166895539, 0.0010683348518796265, 0.001961555622983724, 0.000902584521099925, 0.0009866778855212033, 0.0009640523931011558, 0.0014899327652528882, 0.0013688683975487947, 0.0009729709126986563, 0.000968058331636712, 0.001231376634677872, 0.0013764520408585668, 0.001003533834591508, 0.001762463478371501, 0.0009674945031292737, 0.001204654952744022, 0.0013816923019476235, 0.0010482997167855501, 0.0009112973930314183, 0.0011606206535361707, 0.0011049019522033632, 0.001464352069888264, 0.001258739153854549, 0.0008972728392109275, 0.0012475756811909378, 0.0013749407371506095, 0.001215952739585191, 0.0014738545287400484, 0.001305634737946093, 0.0010521845542825758, 0.0013981274678371847, 0.0011483229463919997, 0.0010331152298022062, 0.0013357644784264266, 0.0009677640337031335, 0.0012233079178258777, 0.0012757920194417238, 0.0011977166577707976, 0.0013564392575062811, 0.0008771553111728281, 0.0011822347587440163, 0.001394367660395801, 0.0011718497262336314, 0.0017108191386796534, 0.0010062829533126205, 0.0011427535791881382, 0.0009751233155839145, 0.0009074282716028392, 0.0010604426497593522, 0.0009840058628469706, 0.0014080137480050325, 0.001532173657324165, 0.0010982019011862576, 0.0014062509289942682, 0.001242252386873588, 0.0009992063569370657, 0.001232035894645378, 0.001053673739079386, 0.0013269227929413319, 0.0012058803986292332, 0.001012737600831315, 0.0012484664330258965, 0.0008894296188373119, 0.0009440188587177545, 0.001344785327091813, 0.0014441291568800807, 0.0014149333001114428, 0.00171914097154513, 0.0013115304172970355, 0.00121967465383932, 0.0012980379979126155, 0.0013014277792535722, 0.0008974193769972771, 0.0016082263900898397, 0.0008966784225776792, 0.000984953629085794, 0.0010308788623660803, 0.0012600908521562815, 0.000926399021409452, 0.0010933250014204532, 0.001133585348725319, 0.0011430891754571348, 0.00105166039429605, 0.0012796084920410067, 0.0010095445031765848, 0.0010501580545678735, 0.001414608210325241, 0.0013024993240833282, 0.000882702530361712, 0.0012856979155912995, 0.0014926177100278437, 0.0008880419190973043, 0.0018524573533795774, 0.0011980576091445982, 0.0011934119393117726, 0.0012570324470289052, 0.0013773792888969183, 0.0013001346960663795, 0.0014112957287579775, 0.00098752198391594, 0.0013465735828503966, 0.0009411605424247682, 0.0009984897333197296, 0.0016921794158406556, 0.001838949858210981, 0.0008828384161461145, 0.0017573987133800983, 0.0009772742923814803, 0.0010685890156310052, 0.0011146517936140299, 0.0011247167421970516, 0.0008967556932475418, 0.0018998284940607846, 0.0014913208433426917, 0.0010537875641603023, 0.0010378869192209095, 0.0009355715883430094, 0.001176955207483843, 0.001246234227437526, 0.0010259537084493786, 0.0009213752346113324, 0.0010802537726704031, 0.0008952739590313286, 0.0009864704916253686, 0.001299722003750503, 0.0008820828807074577, 0.0016957286861725152, 0.0009489960502833128, 0.0016272446955554187, 0.0017027535359375179, 0.001271135755814612, 0.0013623867998830974, 0.0013260530540719628, 0.0010043526126537472, 0.0009405036107636988, 0.0009461363370064646, 0.0011315355368424207, 0.0009897754644043744, 0.0013072237488813698, 0.0011505466245580465, 0.001017064758343622, 0.0010503464727662504, 0.000970765802776441, 0.0013263263390399516, 0.0011941084521822631, 0.0012450610229279846, 0.001307108614128083, 0.0014225772465579212, 0.0009170298289973289, 0.0009813524375203997, 0.0009785657457541674, 0.0010124914406333119, 0.0013006357476115227, 0.0010613343329168856, 0.0008935437654145062, 0.0012954316334798932, 0.0014243185287341475, 0.0017070595640689135, 0.0013404301716946065, 0.0010428219684399664, 0.000880539184436202, 0.0018609067774377763, 0.0015925869229249656, 0.001104004681110382, 0.0013363665784709156, 0.0012739938392769545, 0.0012893134844489396, 0.0011582321603782475, 0.0015282448730431497, 0.0014572283253073692, 0.000979647389613092, 0.000885212590219453, 0.0014491307083517313, 0.001469981565605849, 0.0010222244018223137, 0.0009766814182512462, 0.0015779357054270804, 0.0011581250873859972, 0.0008788409468252212, 0.0009536764118820429, 0.0010457581374794245, 0.001332314859610051, 0.0016972269513644278, 0.0012481946323532611, 0.001585670281201601, 0.0015044623287394643, 0.001016417081700638, 0.001170588715467602, 0.0014189546345733106, 0.0015191464335657656, 0.0009525453497190028, 0.001388306322041899, 0.0014893532497808337, 0.0014122332213446498, 0.001297168550081551, 0.0010356248239986598, 0.001108902069972828, 0.0013351517263799906, 0.0009382583375554532, 0.0010775656264740974, 0.0009844504238571972, 0.0017746996018104255, 0.001709001138806343, 0.0010118827340193093, 0.001039151451550424, 0.0013077080366201699, 0.001118162675993517, 0.001379479537717998, 0.0013926069368608296, 0.001989182026591152, 0.0009573894494678825, 0.00150453916285187, 0.001041798503138125, 0.0016655250801704824, 0.000961268407991156, 0.0011148516205139458, 0.0012840863782912493, 0.0019886429654434323, 0.0015386604936793447, 0.0012950132368132472, 0.0012165394145995378, 0.0013599450467154384, 0.001193539472296834, 0.0010860355687327683, 0.0010661586711648852, 0.001439552754163742, 0.0010165435960516334, 0.0011332873837091029, 0.0015163286589086056, 0.001197423756821081, 0.0018518850556574762, 0.0014875266351737082, 0.0019562849192880094, 0.001092552294721827, 0.0010652864293660969, 0.0015270612784661353, 0.0014260184834711254, 0.0014804828097112477, 0.0019216592190787196, 0.001244634244358167, 0.001844216138124466, 0.001373509701807052, 0.001044914242811501, 0.001003154757199809, 0.0010303451563231647, 0.000953949085669592, 0.001170844043372199, 0.0012466530897654593, 0.0010959767969325185, 0.001564408652484417, 0.0013576327473856509, 0.001351903541944921, 0.0014269580715335906, 0.0018066768534481525, 0.0009740123932715505, 0.0009786877199076116, 0.0019421969191171229, 0.001173687051050365, 0.001268200488993898, 0.001542401616461575, 0.0013049390399828553, 0.0011203214817214757, 0.0009549683600198478, 0.0011144058080390096, 0.0008824605029076338, 0.0009193866862915456, 0.0017583565786480904, 0.0019278260879218578, 0.001420935383066535, 0.000992994784610346, 0.000965126120718196, 0.0012826259189751, 0.0011651194072328508, 0.0013818398001603782, 0.0015853002551011741, 0.0009386127348989248, 0.0011995696695521474, 0.0011047935113310814, 0.0012995739234611392, 0.0014864897821098566, 0.0008915507351048291, 0.001185665198136121, 0.0009090704552363604, 0.0009958459413610399, 0.0012062805180903524, 0.0017224001348949969, 0.0012014741078019142, 0.0010839227179531008, 0.0011820116196759045, 0.001341163762845099, 0.001521318161394447, 0.0010454469011165202, 0.0010791803651954979, 0.0012279590300749987, 0.0014701216714456677, 0.0011037710937671363, 0.0010645287693478167, 0.001015978865325451, 0.0013159708469174802, 0.0015081780729815364, 0.0010467536631040275, 0.0010373049299232662, 0.0014666641945950687, 0.001237424265127629, 0.0010826387733686715, 0.0009861025610007346, 0.0010262235591653734, 0.0011026451829820871, 0.0009186952956952155, 0.0008970527560450137, 0.0010291660437360406, 0.0014970864867791533, 0.0014810340944677591, 0.0011742661008611321, 0.0016899375477805734, 0.0011622689780779183, 0.001127730793086812, 0.0011412353487685323, 0.0017215479165315628, 0.00112514931242913, 0.0013454511063173413, 0.0018686619587242603, 0.001056417590007186, 0.0011469961027614772, 0.0015759059460833669, 0.0016090921708382666, 0.001541998761240393, 0.0014496224466711283, 0.001449227100238204, 0.0012816043454222381, 0.0011978623806498945, 0.001180181367089972, 0.001210590620758012, 0.0009717303910292685, 0.0012889549252577126, 0.001308745238929987, 0.0013035458396188915, 0.001095158513635397, 0.0008796396723482758, 0.0013697450631298125, 0.0009481393499299884, 0.0015228503034450114, 0.0008826398116070777, 0.0009508858493063599, 0.0016274561057798564, 0.001817435200791806, 0.0018137195147573948, 0.001921960385516286, 0.0009354165231343359, 0.0013417626614682376, 0.0009406451717950404, 0.00109851811430417, 0.000946777145145461, 0.0011153201048728079, 0.001611779909580946, 0.0014626431511715055, 0.0010832027182914317, 0.0011406214616727084, 0.0011662218603305519, 0.0017607520567253232, 0.0014512399793602526, 0.0009079317969735712, 0.0009707291901577264, 0.0009004763560369611, 0.0010877303429879248, 0.0016274423687718809, 0.0012103362241759896, 0.001075037260306999, 0.0014742750208824873, 0.0011264323256909847, 0.001269455417059362, 0.0009589366964064538, 0.0010410048125777394, 0.0012821756827179343, 0.0018619613838382065, 0.0011830947769340128, 0.0009580950427334756, 0.0009102360054384917, 0.0009059121657628566, 0.0011895170900970697, 0.0008853829058352858, 0.001016551541397348, 0.0013128587743267417, 0.001535701157990843, 0.0011332786234561354, 0.0011762020003516227, 0.0018560705939307809, 0.001244133134605363, 0.0010489218402653933, 0.0010360075975768268, 0.00098362221615389, 0.0012656991311814636, 0.0011260235914960504, 0.0013084252714179456, 0.0019227687153033912, 0.001366717740893364, 0.0011386563710402697, 0.0009532993426546454, 0.0009050507505889982, 0.0013353308313526213, 0.0010994251060765237, 0.0016728414339013398, 0.0013016540906392038, 0.0010626842849887908, 0.001289701962377876, 0.0015227884287014604, 0.0010415492579340935, 0.0011712050763890147, 0.0010678312391974032, 0.0010816901922225952, 0.0016940077766776085, 0.000908894173335284, 0.0011402647069189698, 0.0010055711318273097, 0.002063782943878323, 0.0014320487971417606, 0.0016131369629874825, 0.0013033419381827116, 0.0013276300160214305, 0.0012719939404632896, 0.0008909868483897299, 0.0010556631023064256, 0.0013607003493234515, 0.0013901267084293067, 0.0019837551517412066, 0.0014630998484790325, 0.0011054080969188362, 0.001169222843600437, 0.002097017248161137, 0.0013372810208238661, 0.0012281503004487604, 0.0018586879596114159, 0.0009900208679027855, 0.0014170787180773914, 0.0011529185867402703, 0.0010621129476930946, 0.001159314182586968, 0.0008810815343167633, 0.0009355247893836349, 0.0009871510264929384, 0.001417099847458303, 0.001429421768989414, 0.0010149039153475314, 0.0009664170793257654, 0.0009723708790261298, 0.0013889817637391388, 0.0017636478878557682, 0.0010047662944998592, 0.0012289781880099326, 0.0013678587274625897, 0.0010572868050076067, 0.0014999824343249202, 0.0012235701433382928, 0.0021136316936463118, 0.0008910161268431693, 0.0014939326792955399, 0.001263772021047771, 0.0010740899888332933, 0.001543356105685234, 0.00172721897251904, 0.0010493192821741104, 0.0019534556195139885, 0.0016044400399550796, 0.0014108438626863062, 0.0008885159331839532, 0.0013970750733278692, 0.001106571959098801, 0.0013081732322461903, 0.0017011815216392279, 0.0010482080397196114, 0.0012254916364327073, 0.0013833786360919476, 0.0009162915521301329, 0.0013512028963305056, 0.0010523777164053172, 0.0008827234851196408, 0.0012145253422204405, 0.0011667751532513648, 0.0015417824615724385, 0.0009060234297066927, 0.0010203657438978553, 0.0009661198128014803, 0.0011490998440422118, 0.001042155927279964, 0.0012919685686938465, 0.0008970936760306358, 0.0013902740320190787, 0.0011364631936885417, 0.0010145173873752356, 0.001344371645245701, 0.001267160172574222, 0.0009344893041998148, 0.0009425670723430812, 0.0009097606234718114, 0.0010250377235934138, 0.0009860680438578129, 0.0013575393240898848, 0.001537413860205561, 0.001174885459477082, 0.0013621329562738538, 0.001671972800977528, 0.0010892980208154768, 0.0009103032352868468, 0.0011932276538573205, 0.0017470706952735782, 0.0013904625084251165, 0.0012023770832456648, 0.0016352854436263442, 0.0011411162267904729, 0.001301547687035054, 0.0011001853563357145, 0.0010850050020962954, 0.0009546633518766612, 0.000943871506024152, 0.0009175025334116071, 0.0011895759962499142, 0.0009173506987281144, 0.0011152171937283128, 0.0011787890980485827, 0.001489301270339638, 0.0010495295282453299, 0.001331486040726304, 0.0014632839593105018, 0.001334304513875395, 0.0010145003907382488, 0.0009271331655327231, 0.000926238571992144, 0.0008787426631897688, 0.0009219179046340287, 0.0015165439108386636, 0.001172140589915216, 0.0014251742977648973, 0.001205756183480844, 0.0009965407371055335, 0.001370033889543265, 0.0011419911170378327, 0.0009116425644606352, 0.0012220561038702726, 0.0009482208988629282, 0.0013147477875463665, 0.0014479910605587065, 0.0009446569602005184, 0.0009194653539452702, 0.001193154021166265, 0.0010821483156178147, 0.0016587887075729668, 0.0013791422243230045, 0.0010073251614812762, 0.0011577787226997316, 0.0010910338023677468, 0.0009703030809760094, 0.0010349622752983123, 0.0009980568429455161, 0.0011331977148074657, 0.0009122782794293016, 0.0010970307339448482, 0.0014116113306954503, 0.001012899389024824, 0.0012848267215304077, 0.0015490216901525855, 0.0009173720609396696, 0.0012771193287335336, 0.0016339721623808146, 0.0011303540377411991, 0.0012954323319718242, 0.0013329494395293295, 0.0011941920965909958, 0.001161424006568268, 0.0013900637859478593, 0.0009694423351902515, 0.0014741680934093893, 0.000994903821265325, 0.0009333874040748924, 0.001596134970895946, 0.0011356467148289084, 0.0009734656778164208, 0.0018089392688125372, 0.0008858611108735204, 0.0014562662690877914, 0.000926182750845328, 0.0010497343027964234, 0.0009731422469485551, 0.0014727534726262093, 0.0016728059272281826, 0.0010973007010761648, 0.001150010823039338, 0.0009550187678541988, 0.001006629114272073, 0.0008964319422375411, 0.0015569293755106628, 0.0013381215394474566, 0.0012864086893387139, 0.0015164634096436203, 0.0010608042357489467, 0.0018138904706574976, 0.0010974069591611624, 0.0016614104970358312, 0.001064947689883411, 0.0011395855690352619, 0.0010642297856975347, 0.0018676542676985264, 0.0013840858591720462, 0.001305634097661823, 0.0012052934616804123, 0.0012260674848221242, 0.001464760978706181, 0.0014095448423177004, 0.0012585836229845881, 0.0011272255796939135, 0.0014191672671586275, 0.0011543375730980188, 0.0009603001526556909, 0.0010251904022879899, 0.0010580545931588858, 0.000909719878109172, 0.0011220952437724918, 0.0016960112261585891, 0.001215448894072324, 0.001162947592092678, 0.001760908227879554, 0.0011737525928765535, 0.0009589492983650416, 0.0014569886843673885, 0.000977994641289115, 0.0018266106490045786, 0.0010910670389421284, 0.0013167199795134366, 0.001319483679253608, 0.0011578574194572866, 0.0010436275333631784, 0.0010093999735545367, 0.0011471810285001993, 0.0010253521031700075, 0.0014100219123065472, 0.0008761990757193416, 0.0010188474552705884, 0.0011270067770965397, 0.0021803703857585788, 0.0009245336696039885, 0.0011324882216285914, 0.0009508170187473297, 0.0009019419085234404, 0.0009997712331824005, 0.0015308830770663917, 0.0015964061603881419, 0.0009245909459423274, 0.0009578088647685945, 0.0011014112387783825, 0.001520735677331686, 0.001227344386279583, 0.0010020217159762979, 0.0015567729133181274, 0.001487088156864047, 0.001159358595032245, 0.0011090069310739636, 0.0011234835255891085, 0.0012990838731639087, 0.0012683372478932142, 0.0009665495599620044, 0.0009428953926544636, 0.0013083178200758994, 0.0012905940529890358, 0.0009137271845247597, 0.0011290078982710838, 0.0010911636636592448, 0.0011851199087686837, 0.0010849169339053333, 0.0011433014587964863, 0.0012026893091388047, 0.0012018269335385412, 0.0013767111813649535, 0.0009044664329849184, 0.0013448208919726312, 0.0013205943396314979, 0.0013383430778048933, 0.0012735354830510914, 0.0012096312129870057, 0.0009164442890323699, 0.0011873892799485475, 0.0014571453793905675, 0.002193447551690042, 0.0012926605413667858, 0.0009515153360553086, 0.0010375328711234033, 0.0009086194622796029, 0.0009167220268864185, 0.0010976636258419603, 0.0022295551607385278, 0.0010831274557858706, 0.0011429721780586988, 0.0008876661886461079, 0.0015931475209072232, 0.002152437635231763, 0.001092445949325338, 0.001831830886658281, 0.0012540841707959771, 0.0011109486513305455, 0.0014620234142057598, 0.0011472909536678344, 0.001285624282900244, 0.0018401957931928337, 0.0016118779312819242, 0.0013843595515936613, 0.0016605604905635118, 0.0015806104056537151, 0.0011883228144142777, 0.0010058113257400692, 0.0014510691980831325, 0.0008872277685441077, 0.0014070927281863987, 0.0015627927496097982, 0.0011095044610556215, 0.0009108181693591177, 0.0013792065437883139, 0.0010724127059802413, 0.0009212467411998659, 0.001540407247375697, 0.0010358469444327056, 0.0009157290915027261, 0.0009400160342920572, 0.0013051180867478251, 0.0009600351913832128, 0.0015431560459546745, 0.0009986961085814983, 0.0009334291389677674, 0.0012772479676641524, 0.000897598365554586, 0.0010226157610304654, 0.002585855661891401, 0.000945407198742032, 0.0020359853515401483, 0.0015950046945363283, 0.0010256761452183127, 0.0012126395013183355, 0.0011227332288399339, 0.0011987421894446015, 0.000884492794284597, 0.001023545570205897, 0.0009182808571495116, 0.0009798006794881076, 0.0009325234568677843, 0.0010452067072037607, 0.001193335046991706, 0.0014056533691473305, 0.0013201541732996702, 0.0021212922874838114, 0.0010534752800595015, 0.00172583811217919]\n"
     ]
    }
   ],
   "source": [
    "print(val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contruct a class and define the constructor \n",
    "class StepbyStep(object):\n",
    "    def __init__(self,model,loss_fn, optimizer):\n",
    "        # Define some attributes to use them later \n",
    "        self.model = model \n",
    "        self.loss_fn = loss_fn \n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        # Automatically decided the device to use\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        # Send the model to the device\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        # These attributes are defined here, but since they are \n",
    "        # not available at the moment of creation, we keep them None\n",
    "        self.train_loader = None\n",
    "        self.val_loader = None \n",
    "        self.writer = None\n",
    "        \n",
    "        # These attributes are going to be computed internally\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        self.total_epochs = 0 \n",
    "        \n",
    "        # Create the train_step function for our model/loss function and optimizer \n",
    "        # there are no arguments there. It makes use of the class attributes directly \n",
    "        self.train_step = self._make_train_step()\n",
    "        \n",
    "        # Create the val_step function for model and loss\n",
    "        self.val_step = self._make_val_step()\n",
    "        \n",
    "    def to(self, device):\n",
    "        \"\"\"Function to let the user specify the device.\"\"\"\n",
    "        self.device = device\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "    \n",
    "    def set_loaders(self, train_loader, val_loader=None):\n",
    "        \"\"\"Let the user set the loaders.\"\"\"\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "    \n",
    "    def set_tensorboard(self, name, folder='runs'):\n",
    "        \"\"\"Let the user to set the tensorboard.\"\"\"\n",
    "        suffix = datetime.datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "        self.writer = SummaryWriter(\n",
    "            '{}/{}_{}'.format(folder, name, suffix)\n",
    "        )\n",
    "        \n",
    "    def _make_train_step(self):\n",
    "        # Builds function that performs a step in the train loop \n",
    "        def perform_train_step(x,y):\n",
    "        \n",
    "            # Set the model to TRAIN mode\n",
    "            self.model.train()\n",
    "        \n",
    "            # Step1: Compute the model's predicition - forward pass\n",
    "            yhat = self.model(x)\n",
    "        \n",
    "            # Step2: Compute the loss\n",
    "            loss = self.loss_fn(yhat, y)\n",
    "        \n",
    "            # Step3: Compute gradients for \"b\" and \"w\" parameters\n",
    "            loss.backward()\n",
    "        \n",
    "            # Step4: Updates parameters using gradients and the learning rate\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "        \n",
    "            #print(model.state_dict())\n",
    "            # Return the loss \n",
    "            return loss.item()\n",
    "        #Return the function that will be called inside the train loop\n",
    "        return perform_train_step\n",
    "    \n",
    "    def _make_val_step(self):\n",
    "        # Build function that perform a step in the validation loop\n",
    "        def perform_val_step(x,y):\n",
    "            # Set the model to evaluation mode\n",
    "            self.model.eval()\n",
    "        \n",
    "            # Step 1: Compute the model's prediciton-forward pass\n",
    "            yhat = self.model(x)\n",
    "        \n",
    "            # Step 2: Compute the loss\n",
    "            loss = self.loss_fn(yhat, y)\n",
    "        \n",
    "            # We don't need to compute gradients and update the parameter here\n",
    "            # since we don't update parameters during evaluation.\n",
    "            return loss.item()\n",
    "        return perform_val_step\n",
    "    \n",
    "    def _mini_batch(self, validation=False):\n",
    "        \"\"\"Could be used for training and validation.\n",
    "        \n",
    "        we use the argument `validation` to define which loader be used \n",
    "        \"\"\"\n",
    "        if validation:\n",
    "            dataloader = self.val_loader\n",
    "            step = self.val_step\n",
    "        else:\n",
    "            dataloader = self.train_loader\n",
    "            step = self.train_step\n",
    "        \n",
    "        if dataloader is None:\n",
    "            return None \n",
    "        \n",
    "        mini_batch_losses = []\n",
    "        for x_batch, y_batch in dataloader:\n",
    "            # Send the mini batch to device\n",
    "            x_batch = x_batch.to(self.device)\n",
    "            y_batch = y_batch.to(self.device)\n",
    "            \n",
    "            # Perform the step\n",
    "            mini_batch_loss = step(x_batch, y_batch)\n",
    "            mini_batch_losses.append(mini_batch_loss)\n",
    "        # Compute the average loss over all mini-batches\n",
    "        loss = np.mean(mini_batch_losses)\n",
    "        return loss\n",
    "    \n",
    "    def set_seed(self, seed=42):\n",
    "        \"\"\"Let the user to set the seed for reproducibility.\"\"\"\n",
    "        # https://pytorch.org/docs/stable/notes/randomness.html\n",
    "        torch.backends.cudnn.derministic = True\n",
    "        torch.backends.cudnn.benchmark =False\n",
    "        \n",
    "        #Sets the seed for generating random numbers.\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    def train(self, n_epochs, seed=42):\n",
    "        \"\"\"Define the train loops.\"\"\"\n",
    "        \n",
    "        # Set the seed for reproducibility\n",
    "        self.set_seed(seed)\n",
    "        \n",
    "        losses = []\n",
    "        val_losses = []\n",
    "        for epoch in range(n_epochs):\n",
    "            # Keep track of the number of epochs\n",
    "            self.total_epochs+=1\n",
    "            \n",
    "            # Call the mini batch function\n",
    "            loss = self._mini_batch(validation=False)\n",
    "            self.losses.append(loss)\n",
    "    \n",
    "            # Validation - no gradients in validation\n",
    "            # Use `torch.no_grad()` this context manager \n",
    "            # to disable any gradient computation\n",
    "            with torch.no_grad():\n",
    "                val_loss = self._mini_batch(validation=True)\n",
    "                self.val_losses.append(val_loss)\n",
    "\n",
    "            # If a SummarWriter has been set...\n",
    "            if self.writer:\n",
    "                scalars = {'training':loss}\n",
    "                if val_loss is not None:\n",
    "                    scalars.update({'validation': val_loss})\n",
    "                    \n",
    "                # Add scalars \n",
    "                self.writer.add_scalars(\n",
    "                   main_tag = 'loss',\n",
    "                   tag_scalar_dict = scalars,\n",
    "                global_step=epoch)\n",
    "        \n",
    "        if self.writer:\n",
    "            #Flushes the writer\n",
    "            self.writer.flush()\n",
    "            \n",
    "    def save_checkpoint(self, filename):\n",
    "        \"\"\"Builds dictionary with all elements for resuming training\"\"\"\n",
    "        checkpoint ={\n",
    "            'epoch': self.total_epochs,\n",
    "            'model_state_dict': self.model_state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer_state_dict(),\n",
    "            'loss': self.losses,\n",
    "            'val_loss':sel.val_losses\n",
    "        }\n",
    "        torch.save(checkpoint, filename)\n",
    "        \n",
    "    def load_checkpoint(self, filename):\n",
    "        \"\"\"Function to let the user load the checkpoint.\"\"\"\n",
    "        \n",
    "        # Load the checkpoint file to dictionary\n",
    "        checkpoint = torch.load(filename)\n",
    "        \n",
    "        # Restore the saved parameter \n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        self.total_epochs=checkpoint['epoch']\n",
    "        self.losses=checkpoint['loss']\n",
    "        self.val_losse=checkpoint['val_loss']\n",
    "        \n",
    "    def predict(self,x):\n",
    "        \"\"\"Construct the predict funciton.\"\"\"\n",
    "        \n",
    "        # Set the model to eval mode for prediction\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Take a numpy input and make it a float tensor \n",
    "        x_tensor = torch.as_tensor(x).float()\n",
    "        \n",
    "        # Send input to device and use model for prediction\n",
    "        y_hat_tensor = self.model(x_tensor.to(self.device))\n",
    "        \n",
    "        # Set the model back to train mode\n",
    "        self.mode.train()\n",
    "        \n",
    "        # Detaches it, bring it to CPU and back to numpy\n",
    "        return y_hat_tensor.detach().cpu().numpy()\n",
    "    \n",
    "    def plot_lossed(self):\n",
    "        \"\"\"Draw some basic plots.\"\"\"\n",
    "        fig = plt.figure(figsiz=(10,4))\n",
    "        \n",
    "        plt.plot(self.losses, label=\"Training Loss\", c='b')\n",
    "        \n",
    "        if self.val_loader:\n",
    "            plt.plot(self.val_losses, label=\"Validation Loss\", c='r')\n",
    "        plt.yscale('log')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        return fig \n",
    "    \n",
    "    def add_graph():\n",
    "        \"\"\"Add tensorboard used graph.\"\"\"\n",
    "        if self.train_loader and self.writer:\n",
    "            x_dummy, y_dummy =next(iter(self.train_loader))\n",
    "            self.writer.add_graph(self.model, x_dummy.to(self.device))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
