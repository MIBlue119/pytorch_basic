{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the difference between `nn.ModuleList()` and `nn.Sequential()`?\n",
    "- nn.Module/nn.ModuleList/nn.Sequential are all containers that we could add module in it "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `nn.Module`\n",
    "- torch docs: https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module\n",
    "- Base class for all neural network modules \n",
    "- The module addded into `nn.ModuleList` would automatically  be registered at module networks \n",
    "    and the module's parameters is automatically appended into networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.hooks.RemovableHandle object at 0x7fea4080ced0>\n",
      "<torch.utils.hooks.RemovableHandle object at 0x7fea4080ced0>\n",
      "<torch.utils.hooks.RemovableHandle object at 0x7fea4080ced0>\n",
      "Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "-------------Module parameters----------------\n",
      "520\n",
      "-------------Input Grad ----------------\n",
      "torch.Size([1, 1, 224, 224])\n",
      "-------------Output Grad ----------------\n",
      "torch.Size([20, 220, 220])\n",
      "\n",
      "\n",
      "Conv2d(20, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "-------------Module parameters----------------\n",
      "10020\n",
      "-------------Input Grad ----------------\n",
      "torch.Size([1, 20, 220, 220])\n",
      "-------------Output Grad ----------------\n",
      "torch.Size([20, 216, 216])\n",
      "\n",
      "\n",
      "MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "-------------Module parameters----------------\n",
      "0\n",
      "-------------Input Grad ----------------\n",
      "torch.Size([1, 20, 216, 216])\n",
      "-------------Output Grad ----------------\n",
      "torch.Size([20, 216, 216])\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 20, 216, 216])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct a basic Network and add hook at forward\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "\n",
    "class NetSample(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5)\n",
    "        self.conv2 = nn.Conv2d(20,20,5)\n",
    "        self.max_pool = nn.MaxPool2d(3,stride=1,padding=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.max_pool(x)\n",
    "        return x \n",
    "\n",
    "def hook_fn(module, input, output):\n",
    "    \"\"\"Ref:https://blog.paperspace.com/pytorch-hooks-gradient-clipping-debugging/\"\"\"\n",
    "    print(module)\n",
    "    print(\"-------------Module parameters----------------\")\n",
    "    neles = sum( [ param.nelement() for param in module.parameters()])\n",
    "    print(neles)\n",
    "    print(\"-------------Input Grad ----------------\")\n",
    "\n",
    "    for grad in input:\n",
    "        try:\n",
    "            print(grad.shape)\n",
    "        except AttributeError:\n",
    "            print(\"None found for Gradient\")\n",
    "    print(\"-------------Output Grad ----------------\")\n",
    "    for grad in output:\n",
    "        try: \n",
    "            print(grad.shape)\n",
    "        except AttributeError:\n",
    "            print(\"None found for Gradient\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "net = NetSample()\n",
    "# Iterate the childern to register the forward hook\n",
    "for i in net.children():\n",
    "    print(i.register_forward_hook(hook_fn))\n",
    "\n",
    "input = torch.randn(1,1,224,224)\n",
    "net(input).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NetWithPythonBuiltInList()\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "## Don't use Python built in list, the module appened in  builted in list could be registered at the networks\n",
    "\n",
    "class NetWithPythonBuiltInList(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NetWithPythonBuiltInList,self).__init__()\n",
    "        self.linears = [nn.Linear(10,10) for i in range(2)]\n",
    "    \n",
    "    def forward(self,x):\n",
    "        for linear_ele in self.linears:\n",
    "            x = linear_ele(x)\n",
    "        return x \n",
    "\n",
    "net = NetWithPythonBuiltInList()\n",
    "print(net)\n",
    "print(list(net.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `nn.ModuleList`\n",
    "- torch docs: https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html#torch.nn.ModuleList\n",
    "- Could be indexed like a regular Python List "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linears.0 Linear(in_features=10, out_features=10, bias=True)\n",
      "<torch.utils.hooks.RemovableHandle object at 0x7fea613dff90>\n",
      "linears.1 Linear(in_features=10, out_features=10, bias=True)\n",
      "<torch.utils.hooks.RemovableHandle object at 0x7fea710ad850>\n",
      "linears.2 Linear(in_features=10, out_features=10, bias=True)\n",
      "<torch.utils.hooks.RemovableHandle object at 0x7fea710acc90>\n",
      "linears.3 Linear(in_features=10, out_features=10, bias=True)\n",
      "<torch.utils.hooks.RemovableHandle object at 0x7fea70fa0290>\n",
      "linears.4 Linear(in_features=10, out_features=10, bias=True)\n",
      "<torch.utils.hooks.RemovableHandle object at 0x7fea613e33d0>\n",
      "linears.5 Linear(in_features=10, out_features=10, bias=True)\n",
      "<torch.utils.hooks.RemovableHandle object at 0x7fea613e3550>\n",
      "linears.6 Linear(in_features=10, out_features=10, bias=True)\n",
      "<torch.utils.hooks.RemovableHandle object at 0x7fea710c8450>\n",
      "linears.7 Linear(in_features=10, out_features=10, bias=True)\n",
      "<torch.utils.hooks.RemovableHandle object at 0x7fea710c8f50>\n",
      "linears.8 Linear(in_features=10, out_features=10, bias=True)\n",
      "<torch.utils.hooks.RemovableHandle object at 0x7fea710c8290>\n",
      "linears.9 Linear(in_features=10, out_features=10, bias=True)\n",
      "<torch.utils.hooks.RemovableHandle object at 0x7fea710c8f10>\n",
      "Linear(in_features=10, out_features=10, bias=True)\n",
      "-------------Module parameters----------------\n",
      "110\n",
      "-------------Input Grad ----------------\n",
      "torch.Size([1, 10])\n",
      "-------------Output Grad ----------------\n",
      "torch.Size([10])\n",
      "\n",
      "\n",
      "Linear(in_features=10, out_features=10, bias=True)\n",
      "-------------Module parameters----------------\n",
      "110\n",
      "-------------Input Grad ----------------\n",
      "torch.Size([1, 10])\n",
      "-------------Output Grad ----------------\n",
      "torch.Size([10])\n",
      "\n",
      "\n",
      "Linear(in_features=10, out_features=10, bias=True)\n",
      "-------------Module parameters----------------\n",
      "110\n",
      "-------------Input Grad ----------------\n",
      "torch.Size([1, 10])\n",
      "-------------Output Grad ----------------\n",
      "torch.Size([10])\n",
      "\n",
      "\n",
      "Linear(in_features=10, out_features=10, bias=True)\n",
      "-------------Module parameters----------------\n",
      "110\n",
      "-------------Input Grad ----------------\n",
      "torch.Size([1, 10])\n",
      "-------------Output Grad ----------------\n",
      "torch.Size([10])\n",
      "\n",
      "\n",
      "Linear(in_features=10, out_features=10, bias=True)\n",
      "-------------Module parameters----------------\n",
      "110\n",
      "-------------Input Grad ----------------\n",
      "torch.Size([1, 10])\n",
      "-------------Output Grad ----------------\n",
      "torch.Size([10])\n",
      "\n",
      "\n",
      "Linear(in_features=10, out_features=10, bias=True)\n",
      "-------------Module parameters----------------\n",
      "110\n",
      "-------------Input Grad ----------------\n",
      "torch.Size([1, 10])\n",
      "-------------Output Grad ----------------\n",
      "torch.Size([10])\n",
      "\n",
      "\n",
      "Linear(in_features=10, out_features=10, bias=True)\n",
      "-------------Module parameters----------------\n",
      "110\n",
      "-------------Input Grad ----------------\n",
      "torch.Size([1, 10])\n",
      "-------------Output Grad ----------------\n",
      "torch.Size([10])\n",
      "\n",
      "\n",
      "Linear(in_features=10, out_features=10, bias=True)\n",
      "-------------Module parameters----------------\n",
      "110\n",
      "-------------Input Grad ----------------\n",
      "torch.Size([1, 10])\n",
      "-------------Output Grad ----------------\n",
      "torch.Size([10])\n",
      "\n",
      "\n",
      "Linear(in_features=10, out_features=10, bias=True)\n",
      "-------------Module parameters----------------\n",
      "110\n",
      "-------------Input Grad ----------------\n",
      "torch.Size([1, 10])\n",
      "-------------Output Grad ----------------\n",
      "torch.Size([10])\n",
      "\n",
      "\n",
      "Linear(in_features=10, out_features=10, bias=True)\n",
      "-------------Module parameters----------------\n",
      "110\n",
      "-------------Input Grad ----------------\n",
      "torch.Size([1, 10])\n",
      "-------------Output Grad ----------------\n",
      "torch.Size([10])\n",
      "\n",
      "\n",
      "Linear(in_features=10, out_features=10, bias=True)\n",
      "-------------Module parameters----------------\n",
      "110\n",
      "-------------Input Grad ----------------\n",
      "torch.Size([1, 10])\n",
      "-------------Output Grad ----------------\n",
      "torch.Size([10])\n",
      "\n",
      "\n",
      "Linear(in_features=10, out_features=10, bias=True)\n",
      "-------------Module parameters----------------\n",
      "110\n",
      "-------------Input Grad ----------------\n",
      "torch.Size([1, 10])\n",
      "-------------Output Grad ----------------\n",
      "torch.Size([10])\n",
      "\n",
      "\n",
      "Linear(in_features=10, out_features=10, bias=True)\n",
      "-------------Module parameters----------------\n",
      "110\n",
      "-------------Input Grad ----------------\n",
      "torch.Size([1, 10])\n",
      "-------------Output Grad ----------------\n",
      "torch.Size([10])\n",
      "\n",
      "\n",
      "Linear(in_features=10, out_features=10, bias=True)\n",
      "-------------Module parameters----------------\n",
      "110\n",
      "-------------Input Grad ----------------\n",
      "torch.Size([1, 10])\n",
      "-------------Output Grad ----------------\n",
      "torch.Size([10])\n",
      "\n",
      "\n",
      "Linear(in_features=10, out_features=10, bias=True)\n",
      "-------------Module parameters----------------\n",
      "110\n",
      "-------------Input Grad ----------------\n",
      "torch.Size([1, 10])\n",
      "-------------Output Grad ----------------\n",
      "torch.Size([10])\n",
      "\n",
      "\n",
      "Linear(in_features=10, out_features=10, bias=True)\n",
      "-------------Module parameters----------------\n",
      "110\n",
      "-------------Input Grad ----------------\n",
      "torch.Size([1, 10])\n",
      "-------------Output Grad ----------------\n",
      "torch.Size([10])\n",
      "\n",
      "\n",
      "Linear(in_features=10, out_features=10, bias=True)\n",
      "-------------Module parameters----------------\n",
      "110\n",
      "-------------Input Grad ----------------\n",
      "torch.Size([1, 10])\n",
      "-------------Output Grad ----------------\n",
      "torch.Size([10])\n",
      "\n",
      "\n",
      "Linear(in_features=10, out_features=10, bias=True)\n",
      "-------------Module parameters----------------\n",
      "110\n",
      "-------------Input Grad ----------------\n",
      "torch.Size([1, 10])\n",
      "-------------Output Grad ----------------\n",
      "torch.Size([10])\n",
      "\n",
      "\n",
      "Linear(in_features=10, out_features=10, bias=True)\n",
      "-------------Module parameters----------------\n",
      "110\n",
      "-------------Input Grad ----------------\n",
      "torch.Size([1, 10])\n",
      "-------------Output Grad ----------------\n",
      "torch.Size([10])\n",
      "\n",
      "\n",
      "Linear(in_features=10, out_features=10, bias=True)\n",
      "-------------Module parameters----------------\n",
      "110\n",
      "-------------Input Grad ----------------\n",
      "torch.Size([1, 10])\n",
      "-------------Output Grad ----------------\n",
      "torch.Size([10])\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-2.3160,  0.7772, -0.7059, -0.1074, -0.4508,  0.4547,  0.5520, -0.2393,\n",
       "         -0.2101,  1.0877]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ModuleList can act as an iterable or be indexed using ints\n",
    "\n",
    "class MyModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModule,self).__init__()\n",
    "        self.linears = nn.ModuleList([ nn.Linear(10,10) for i in range(10) ]) \n",
    "    \n",
    "    def forward(self,x):\n",
    "        for index, linear_ele in enumerate(self.linears):\n",
    "            self.linears[index//2](x) + linear_ele(x)\n",
    "        return x \n",
    "\n",
    "def hook_fn(module, input, output):\n",
    "    \"\"\"Ref:https://blog.paperspace.com/pytorch-hooks-gradient-clipping-debugging/\"\"\"\n",
    "    print(module)\n",
    "    print(\"-------------Module parameters----------------\")\n",
    "    neles = sum( [ param.nelement() for param in module.parameters()])\n",
    "    print(neles)\n",
    "    print(\"-------------Input Grad ----------------\")\n",
    "\n",
    "    for grad in input:\n",
    "        try:\n",
    "            print(grad.shape)\n",
    "        except AttributeError:\n",
    "            print(\"None found for Gradient\")\n",
    "    print(\"-------------Output Grad ----------------\")\n",
    "    for grad in output:\n",
    "        try: \n",
    "            print(grad.shape)\n",
    "        except AttributeError:\n",
    "            print(\"None found for Gradient\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "net = MyModule()\n",
    "# Iterate the childern to register the forward hook\n",
    "# We may meet some problem to register at ModuleList when we use `.named_childern()`\n",
    "# Same issue: https://stackoverflow.com/questions/69078576/pytorch-hook-function-is-not-executed\n",
    "# a ModuleList only have a single child, which contains multiple sub modules\n",
    "# for name, module in net.named_children():\n",
    "#     print(name, module )\n",
    "    # print(module.register_forward_hook(hook_fn))\n",
    "\n",
    "\n",
    "# We could use `.named_modules` to register\n",
    "# Bute `.named_modules` would recursively return the complete model\n",
    "for name, module in net.named_modules():\n",
    "    if isinstance(module,nn.Linear):\n",
    "        print(name, module )\n",
    "        print(module.register_forward_hook(hook_fn))\n",
    "\n",
    "input = torch.randn(1,10)\n",
    "net(input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> torch.Size([10, 10])\n",
      "<class 'torch.Tensor'> torch.Size([10])\n",
      "<class 'torch.Tensor'> torch.Size([10, 10])\n",
      "<class 'torch.Tensor'> torch.Size([10])\n",
      "<class 'torch.Tensor'> torch.Size([10, 10])\n",
      "<class 'torch.Tensor'> torch.Size([10])\n",
      "<class 'torch.Tensor'> torch.Size([10, 10])\n",
      "<class 'torch.Tensor'> torch.Size([10])\n",
      "<class 'torch.Tensor'> torch.Size([10, 10])\n",
      "<class 'torch.Tensor'> torch.Size([10])\n",
      "<class 'torch.Tensor'> torch.Size([10, 10])\n",
      "<class 'torch.Tensor'> torch.Size([10])\n",
      "<class 'torch.Tensor'> torch.Size([10, 10])\n",
      "<class 'torch.Tensor'> torch.Size([10])\n",
      "<class 'torch.Tensor'> torch.Size([10, 10])\n",
      "<class 'torch.Tensor'> torch.Size([10])\n",
      "<class 'torch.Tensor'> torch.Size([10, 10])\n",
      "<class 'torch.Tensor'> torch.Size([10])\n",
      "<class 'torch.Tensor'> torch.Size([10, 10])\n",
      "<class 'torch.Tensor'> torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "# Iterate print the parameters\n",
    "for param in net.parameters():\n",
    "    print(type(param.data), param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net4(\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=5, out_features=10, bias=True)\n",
      "    (1): Linear(in_features=10, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "linears.0.weight torch.Size([10, 5])\n",
      "linears.0.bias torch.Size([10])\n",
      "linears.1.weight torch.Size([10, 10])\n",
      "linears.1.bias torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "# Use nn.ModuleList with index \n",
    "# - The module's parameters in the nn.ModuleList are shareable even thouth the module is called multiple times\n",
    "\n",
    "class net4(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(net4,self).__init__()\n",
    "        self.linears = nn.ModuleList([nn.Linear(5,10), nn.Linear(10,10)])\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.linears[0](x)\n",
    "        x = self.linears[1](x)\n",
    "        x = self.linears[1](x)\n",
    "        return x \n",
    "\n",
    "net = net4()\n",
    "\n",
    "print(net)\n",
    "\n",
    "\n",
    "#Iteratively print the module \n",
    "for name, param in net.named_parameters():\n",
    "    print(name, param.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `nn.Sequential`\n",
    "- Implement forward function in it\n",
    "- The module in the `nn.Sequential` is list by the order, so we should make sure the previous module output size is the same as the next module size input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NetWithSequential(\n",
      "  (block): Sequential(\n",
      "    (0): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(20, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (3): ReLU()\n",
      "  )\n",
      ")\n",
      "torch.Size([64, 216, 216])\n"
     ]
    }
   ],
   "source": [
    "# Example of using nn.Sequential\n",
    "class NetWithSequential(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NetWithSequential,self).__init__()\n",
    "        self.block = nn.Sequential( \n",
    "            nn.Conv2d(1,20,5),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(20,64,5),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.block(x)\n",
    "        return x \n",
    "\n",
    "\n",
    "net = NetWithSequential()\n",
    "print(net)\n",
    "\n",
    "dummy_input = torch.randn(1,224,224)\n",
    "print(net(dummy_input).shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NetWithSequentialOrderDict(\n",
      "  (block): Sequential(\n",
      "    (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (relu1): ReLU()\n",
      "    (conv2): Conv2d(20, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (relu2): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Example of using Sequential with OrderedDict \n",
    "\n",
    "import collections \n",
    "\n",
    "class NetWithSequentialOrderDict(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NetWithSequentialOrderDict,self).__init__()\n",
    "\n",
    "        # We use `collections.OrderedDict` to specify the name of every module \n",
    "        self.block = nn.Sequential( \n",
    "            collections.OrderedDict([\n",
    "             ('conv1',nn.Conv2d(1,20,5) ),\n",
    "             ('relu1',nn.ReLU()),\n",
    "             ('conv2',nn.Conv2d(20,64,5)),\n",
    "             ('relu2',nn.ReLU())\n",
    "            ])\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.block(x)\n",
    "        return x \n",
    "\n",
    "\n",
    "net = NetWithSequentialOrderDict()\n",
    "print(net)\n",
    "\n",
    "# print(list(net.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "['B', 'C', 'D', 'E', 'F', 'G', 'H']\n",
      "I\n",
      "A\n",
      "B\n",
      "['C', 'D', 'E', 'F', 'G', 'H', 'I']\n",
      "['123', '3434', '5', '123', '3434', '5']\n",
      "['123', '3434', '5']\n"
     ]
    }
   ],
   "source": [
    "# What is `*` in python ?\n",
    "# 1. A method to unpack a list\n",
    "#    - https://www.learncodewithmike.com/2019/12/python-unpacking.html\n",
    "# 2. When we don't know the accurate numbers of argument to pass \n",
    "#    - https://developer.aliyun.com/article/282239\n",
    "\n",
    "\n",
    "# Example 1. A method to unpack a list\n",
    "letters = [\"A\", \"B\", \"C\", \"D\", \"E\",\"F\",\"G\",\"H\",\"I\"]\n",
    "# An example to fetch the first/last element and use * to pack the other elements\n",
    "first, *other, last = letters \n",
    "print(first)\n",
    "print(other)\n",
    "print(last)\n",
    "\n",
    "# An example to independently define the needed numbers of variables and unpackaging the last elements\n",
    "first,second, *other = letters \n",
    "print(first)\n",
    "print(second)\n",
    "print(other)\n",
    "\n",
    "\n",
    "# Example 2. When we don't know the accurate numbers of argument to pass \n",
    "a = ['123','3434','5']\n",
    "b = ['123','3434','5','123','3434','5']\n",
    "\n",
    "def print_fn(*list_):\n",
    "    for ele in list_:\n",
    "        print(ele)\n",
    "\n",
    "print_fn(b)\n",
    "print_fn(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario to use `nn.ModuleList` and `nn.Sequential`\n",
    "\n",
    "1. Network contains many repeat layers: we may use `for` to  construct them\n",
    "    a. Use `list` to place the module\n",
    "    b. Use `nn.Sequential` to package the list\n",
    "\n",
    "2. We need the past layers output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net7(\n",
      "  (linears): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (1): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (2): Linear(in_features=10, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Ref:https://zhuanlan.zhihu.com/p/64990232\n",
    "class net7(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(net7, self).__init__()\n",
    "        self.linear_list = [nn.Linear(10, 10) for i in range(3)]\n",
    "        self.linears = nn.Sequential(*self.linear_list)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = self.linears(x)\n",
    "        return x\n",
    "\n",
    "net = net7()\n",
    "print(net)\n",
    "# net7(\n",
    "#   (linears): Sequential(\n",
    "#     (0): Linear(in_features=10, out_features=10, bias=True)\n",
    "#     (1): Linear(in_features=10, out_features=10, bias=True)\n",
    "#     (2): Linear(in_features=10, out_features=10, bias=True)\n",
    "#   )\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 20])\n",
      "torch.Size([32, 30])\n",
      "torch.Size([32, 50])\n"
     ]
    }
   ],
   "source": [
    "# Ref: https://zhuanlan.zhihu.com/p/64990232\n",
    "class net8(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(net8, self).__init__()\n",
    "        self.linears = nn.ModuleList([nn.Linear(10, 20), nn.Linear(20, 30), nn.Linear(30, 50)])\n",
    "        self.trace = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.linears:\n",
    "            x = layer(x)\n",
    "            self.trace.append(x)\n",
    "        return x\n",
    "\n",
    "net = net8()\n",
    "input  = torch.randn(32, 10) # input batch size: 32\n",
    "output = net(input)\n",
    "for each in net.trace:\n",
    "    print(each.shape)\n",
    "# torch.Size([32, 20])\n",
    "# torch.Size([32, 30])\n",
    "# torch.Size([32, 50])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "97a2178eda38448c29255f92091ccc3bc527ad6f76c75a99d80bd1de52db8c32"
  },
  "kernelspec": {
   "display_name": "Python 3.7.5 ('denoise_api')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
